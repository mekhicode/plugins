
sudo vi /etc/apt/sources.list.d/cloudera.list
deb [arch=amd64] http://archive.cloudera.com/impala/ubuntu/precise/amd64/impala precise-impala1 contrib
deb-src http://archive.cloudera.com/impala/ubuntu/precise/amd64/impala precise-impala1 contrib

sudo apt-get update
sudo apt-get install impala
sudo apt-get install impala-server 
sudo apt-get install impala-state-store
sudo apt-get install impala-catalog
sudo apt-get install impala-shell

sudo vi /etc/hive/conf/hive-site.xml
<property>
  <name>hive.metastore.local</name>
  <value>false</value>
</property>
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://<hive-metastore-host>:9083</value>
</property>
<property>
  <name>hive.metastore.client.socket.timeout</name>
  <value>3600</value>
</property>

sudo vi /etc/hadoop/conf/hdfs-site.xml
<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>
<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/run/hadoop-hdfs/dn._PORT</value>
</property>
<property>
  <name>dfs.client.file-block-storage-locations.timeout</name>
  <value>3000</value>
</property>
<property>
  <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
  <value>true</value>
</property>

sudo cp /etc/hive/conf/hive-site.xml /etc/impala/conf
sudo cp /etc/hadoop/conf/core-site.xml /etc/impala/conf
sudo cp /etc/hadoop/conf/hdfs-site.xml /etc/impala/conf

sudo chown -R hdfs:root /var/run/hadoop-hdfs/

sudo usermod -a -G hadoop impala

cd /tmp
wget https://downloads.cloudera.com/impala-jdbc/impala-jdbc-0.5-2.zip
unzip impala-jdbc-0.5-2.zip
sudo mkdir /opt/jars
sudo cp impala-jdbc-0.5-2/* /opt/jars
export CLASSPATH=/opt/jars/*.jar:$CLASSPATH

sudo vi /etc/default/impala
IMPALA_STATE_STORE_HOST=<ip>

sudo service impala-state-store restart
sudo service impala-catalog restart
sudo service impala-server restart

ps ax | grep [s]tatestored
ps ax | grep [c]atalogd
ps ax | grep [i]mpalad

http://<impala-node-host>:25000/varz

beeline
> !connect jdbc:hive2://<hive-server2-host>:21050/;auth=noSasl <username> <password> org.apache.hive.jdbc.HiveDriver
> show tables;

vi /tmp/tab1.csv
1,true,123.123,2012-10-24 08:55:00 
2,false,1243.5,2012-10-25 13:40:00
3,false,24453.325,2008-08-22 09:33:21.123
4,false,243423.325,2007-05-12 22:32:21.33454
5,true,243.325,1953-04-22 09:11:33

vi /tmp/tab2.csv
1,true,12789.123
2,false,1243.5
3,false,24453.325
4,false,2423.3254
5,true,243.325
60,false,243565423.325
70,true,243.325
80,false,243423.325
90,true,243.325

sudo -u hdfs hadoop fs -chmod -R 1777 /user/$USER
sudo -u hdfs hadoop fs -chmod -R 1777 /user/hive/warehouse

hdfs dfs -mkdir -p sample_data/tab1 sample_data/tab2 sample_data/tab3
hdfs dfs -put /tmp/tab1.csv /user/$USER/sample_data/tab1
hdfs dfs -put /tmp/tab2.csv /user/$USER/sample_data/tab2

vi /tmp/tab_setup.sql
drop table if exists tab1;
create external table tab1
(
   id int,
   col_1 boolean,
   col_2 double,
   col_3 timestamp
)
row format delimited fields terminated by ','
location '/user/<username>/sample_data/tab1';
drop table if exists tab2;
create external table tab2
(
   id int,
   col_1 boolean,
   col_2 double
)
row format delimited fields terminated by ','
location '/user/<username>/sample_data/tab2';
drop table if exists tab3;
create table tab3
(
   id int,
   col_1 boolean,
   col_2 double,
   month int,
   day int
)
row format delimited fields terminated by ',';

impala-shell -f /tmp/tab_setup.sql

vi /tmp/customer.dat
1|AAAAAAAABAAAAAAA|980124|7135|32946|2452238|2452208|Mr.|Javier|Lewis|Y|9|12|1936|CHILE||Javier.Lewis@VFAxlnZEvOx.org|2452508|
2|AAAAAAAACAAAAAAA|819667|1461|31655|2452318|2452288|Dr.|Amy|Moses|Y|9|4|1966|TOGO||Amy.Moses@Ovk9KjHH.com|2452318|
3|AAAAAAAADAAAAAAA|1473522|6247|48572|2449130|2449100|Miss|Latisha|Hamilton|N|18|9|1979|NIUE||Latisha.Hamilton@V.com|2452313|
4|AAAAAAAAEAAAAAAA|1703214|3986|39558|2450030|2450000|Dr.|Michael|White|N|7|6|1983|MEXICO||Michael.White@i.org|2452361|
5|AAAAAAAAFAAAAAAA|953372|4470|36368|2449438|2449408|Sir|Robert|Moran|N|8|5|1956|FIJI||Robert.Moran@Hh.edu|2452469|

hdfs dfs -mkdir -p sample_data/customer
hdfs dfs -put /tmp/customer.dat /user/$USER/sample_data/customer/customer.dat

vi /tmp/customer_setup.sql
drop table if exists customer;
create external table customer
(
    c_customer_sk             int,
    c_customer_id             string,
    c_current_cdemo_sk        int,
    c_current_hdemo_sk        int,
    c_current_addr_sk         int,
    c_first_shipto_date_sk    int,
    c_first_sales_date_sk     int,
    c_salutation              string,
    c_first_name              string,
    c_last_name               string,
    c_preferred_cust_flag     string,
    c_birth_day               int,
    c_birth_month             int,
    c_birth_year              int,
    c_birth_country           string,
    c_login                   string,
    c_email_address           string,
    c_last_review_date        string
)
row format delimited fields terminated by '|' 
location '/user/<username>/sample_data/customer';
drop table if exists customer_address;
create external table customer_address
(
    ca_address_sk             int,
    ca_address_id             string,
    ca_street_number          string,
    ca_street_name            string,
    ca_street_type            string,
    ca_suite_number           string,
    ca_city                   string,
    ca_county                 string,
    ca_state                  string,
    ca_zip                    string,
    ca_country                string,
    ca_gmt_offset             float,
    ca_location_type          string
)
row format delimited fields terminated by '|' 
location '/user/<username>/sample_data/customer_address';

impala-shell -f /tmp/customer_setup.sql

impala-shell -q 'select count(*) from customer;'

impala-shell
> show databases;
> show tables;
> describe customer;

> select tab1.col_1, max(tab2.col_2), min(tab2.col_2)
  from tab2 join tab1 using (id)
  group by col_1 order by 1 limit 5;

> select tab2.*
  from tab2,
  (select tab1.col_1, max(tab2.col_2) as max_col2
   from tab2, tab1
   where tab1.id = tab2.id
   group by col_1) subquery1
  where subquery1.max_col2 = tab2.col_2;

> insert overwrite table tab3
  select id, col_1, col_2, month(col_3), dayofmonth(col_3)
  from tab1 where year(col_3) = 2012;

> create database external_partitions;
> use external_partitions;
> create table logs (field1 string, field2 string, field3 string)
    partitioned by (year string, month string , day string, host string)
    row format delimited fields terminated by ',';
> insert into logs partition (year="2013", month="07", day="28", host="host1") values ("foo","foo","foo");

hdfs dfs -ls /user/hive/warehouse/external_partitions.db
hdfs dfs -ls /user/hive/warehouse/external_partitions.db/logs/year=2013/month=07/day=28/host=host1/

echo "bar,baz,bletch" > /tmp/dummy_log_data
hdfs dfs -mkdir -p /user/$USER/data/logs/year=2013/month=07/day=28/host=host1
hdfs dfs -put /tmp/dummy_log_data /user/$USER/data/logs/year=2013/month=07/day=28/host=host1

> use external_partitions;
> alter table logs rename to logs_original;
> create external table logs (field1 string, field2 string, field3 string)
  partitioned by (year string, month string, day string, host string)
  row format delimited fields terminated by ','
  location '/user/<username>/data/logs';

> alter table logs add partition (year="2013",month="07",day="28",host="host1");
> refresh logs;
> select * from logs limit 10;

> set hbase_cache_blocks=true;
> set hbase_caching=1000;

sudo vi /etc/default/impala
IMPALA_SERVER_ARGS=" \
    -default_query_options='hbase_cache_blocks=true;hbase_caching=1000;'

hbase shell
> create 'hbasealltypesagg', 'bools', 'ints', 'floats', 'strings'
> enable 'hbasealltypesagg'
> put 'hbasealltypesagg', 'row1', 'bools:bool_col', 'true'
> create 'hbasealltypessmall', 'bools', 'ints', 'floats', 'strings'
> enable 'hbasealltypessmall'
> put 'hbasealltypessmall', '1', 'bools:bool_col', 'true'

hive
> CREATE EXTERNAL TABLE hbasestringids (
  id string,
  bool_col boolean,
  tinyint_col tinyint,
  smallint_col smallint,
  int_col int,
  bigint_col bigint,
  float_col float,
  double_col double,
  date_string_col string,
  string_col string,
  timestamp_col timestamp)
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
'hbase.columns.mapping'=':key,bools:bool_col,ints:tinyint_col,ints:smallint_col,ints:int_col,ints:bigint_col,floats:float_col,floats:double_col,strings:date_string_col,strings:string_col,strings:timestamp_col')
TBLPROPERTIES (
  'hbase.table.name'='hbasealltypesagg');
> CREATE EXTERNAL TABLE hbasealltypessmall(
  id int,
  bool_col boolean,
  tinyint_col tinyint,
  smallint_col smallint,
  int_col int,
  bigint_col bigint,
  float_col float,
  double_col double,
  date_string_col string,
  string_col string,
  timestamp_col timestamp)
STORED BY
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
'hbase.columns.mapping'=':key,bools:bool_col,ints:tinyint_col,ints:smallint_col,ints:int_col,ints:bigint_col,floats:float_col,floats:double_col,strings:date_string_col,strings:string_col,strings:timestamp_col')
TBLPROPERTIES (
  'hbase.table.name'='hbasealltypessmall');

impala-shell
> INVALIDATE METADATA;
> REFRESH table_name;
> select * from hbasestringids where id = '5';
> select * from hbasealltypessmall where id < 5;
> insert into hbasealltypessmall(id,bool_col) values(2,false);

cd /usr/lib/hive/lib
sudo ln -s /usr/lib/impala/lib/parquet-hive-1.0.jar

hive
> CREATE EXTERNAL TABLE trans_txt( 
  pri_acct_no string, 
  mchnt_cd string, 
  trans_id string, 
  mchnt_tp string, 
  acpt_ins_id_cd string, 
  iss_ins_id_cd string, 
  cups_card_in string, 
  cups_sig_card_in string, 
  card_class string, 
  card_cata string, 
  card_attr string, 
  card_brand string, 
  card_prod string, 
  card_lvl string, 
  trans_chnl string, 
  trans_media string, 
  term_id string, 
  trans_at bigint, 
  trans_rcv_ts string)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
LOCATION
  '/user/<username>/trans_txt';
> load data inpath '/user/<username>/trans_txt/*' into table trans_txt;

impala-shell
> CREATE EXTERNAL TABLE trans_parquet( 
  pri_acct_no string, 
  mchnt_cd string, 
  trans_id string, 
  mchnt_tp string, 
  acpt_ins_id_cd string, 
  iss_ins_id_cd string, 
  cups_card_in string, 
  cups_sig_card_in string, 
  card_class string, 
  card_cata string, 
  card_attr string, 
  card_brand string, 
  card_prod string, 
  card_lvl string, 
  trans_chnl string, 
  trans_media string, 
  term_id string, 
  trans_at bigint, 
  trans_rcv_ts string)
STORED AS PARQUETFILE
LOCATION
  '/user/<username>/trans_parquet';
> describe trans_parquet;
> insert overwrite table trans_parquet select * from trans_txt;
> create table trans_parquet_snappy like trans_parquet STORED AS PARQUETFILE;
> set PARQUET_COMPRESSION_CODEC=snappy;
> insert into trans_parquet_snappy select * from trans_txt;

impala-shell
> create table trans_parquet_everything like trans_parquet_snappy;

sudo -u hdfs hadoop distcp -pb /user/hive/warehouse/trans_parquet \
/user/hive/warehouse/trans_parquet_everything

sudo -u hdfs hadoop fs -mv /user/hive/warehouse/trans_parquet_everything/trans_parquet/* \
/user/hive/warehouse/trans_parquet_everything/

hdfs dfs -du -h /user/hive/warehouse

impala-shell
> refresh trans_parquet_everything;
> select count(*) from trans_parquet_everything;

impala-shell
> ALTER TABLE table_name SET FILEFORMAT PARQUETFILE;
> ALTER TABLE table_name SET SERDE 'parquet.hive.serde.ParquetHiveSerDe';
> ALTER TABLE table_name SET FILEFORMAT
  INPUTFORMAT "parquet.hive.DeprecatedParquetInputFormat"
  OUTPUTFORMAT "parquet.hive.DeprecatedParquetOutputFormat";

